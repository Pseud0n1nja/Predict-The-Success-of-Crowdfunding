{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import shuffle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('train_balanced.csv')\n",
    "# df = shuffle(df)\n",
    "# df.to_csv('train_shuffled.csv')\n",
    "df = pd.read_csv('train_shuffled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34561\n",
      "108129\n",
      "0.6803725180108944\n"
     ]
    }
   ],
   "source": [
    "# count the number of successful projects and failed projects\n",
    "succ_num = sum(df['final_status'] == 1)\n",
    "print(succ_num)\n",
    "total_num = df.shape[0]\n",
    "print(total_num)\n",
    "print(1- succ_num/total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "0. make the dataset balanced (optional).\n",
    "1. encode category features.\n",
    "2. encode text features.\n",
    "3. modify the dataset: add 'duration', drop colunms as 'name', 'project_id', etc.\n",
    "4. split dataset to training, dev and test set (90%, 5%, 5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make the dataset balance: half successful projects and half failed projects\n",
    "def balance(df):\n",
    "    # seperate successful projects and failed projects\n",
    "    df_succ = df[df['final_status'] == 1]\n",
    "    df_fail = df[df['final_status'] == 0]\n",
    "    # duplicate successful projects\n",
    "    df_succ_copy = df_succ.copy()\n",
    "    # random select failed projects and its amount equals to 2 times of sucessful projects\n",
    "    df_fail_sel = df_fail.sample(n = succ_num*2)\n",
    "    # concat the 3 dataframes\n",
    "    df_balance = pd.concat([df_succ, df_succ_copy, df_fail_sel], axis=0)\n",
    "    # shuffle the concated dataframe\n",
    "    df_balance = shuffle(df_balance)\n",
    "    return df_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'category' features, label them with values between 0 and n_classes-1\n",
    "def encoder_cat(df, col):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    col_label = le.fit_transform(df[col])\n",
    "    df[col]=pd.Series(col_label)\n",
    "    return le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text features\n",
    "def encoder_text(df, col, min_df=10):\n",
    "    df[col] = df[col].astype(str)\n",
    "    vectorizer = CountVectorizer(min_df=min_df)\n",
    "    vectorizer.fit(df[col])\n",
    "    col_bag_of_words = vectorizer.transform(df[col])\n",
    "    return col_bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify(df):\n",
    "    # add a new colunm ‘duration’\n",
    "    df['duration'] = df['deadline'] - df['launched_at'] \n",
    "    # drop unused colunms\n",
    "    df = df.drop(columns=['project_id', 'name', 'desc', 'keywords', 'deadline', 'state_changed_at', 'created_at', 'launched_at', 'backers_count', \n",
    "                          'final_status'])\n",
    "    encoder_cat(df, 'country')\n",
    "    encoder_cat(df, 'currency')\n",
    "    encoder_cat(df, 'disable_communication')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(df):\n",
    "    n = float(len(df_data))\n",
    "    n_train = int(n * 0.9)\n",
    "    n_dev = int(n * 0.05)\n",
    "    \n",
    "    training_set = df[:n_train]\n",
    "    dev_set = df[n_train : (n_train + n_dev)]\n",
    "    test_set = df[(n_train + n_dev) :]\n",
    "    return training_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "1. Random Forest Classifier Model (decison tree)\n",
    "2. SVM\n",
    "3. Linear Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = modify(df)\n",
    "df_keywords_encoded = encoder_text(df, 'keywords', min_df=10)\n",
    "df_desc_encoded = encoder_text(df, 'desc', min_df=20)\n",
    "df_labels = df['final_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, dev_data, test_data = data_split(df_data)\n",
    "training_kw, dev_kw, test_kw = data_split(df_keywords_encoded)\n",
    "training_desc, dev_desc, test_desc = data_split(df_desc_encoded)\n",
    "\n",
    "training_Y, dev_Y, test_Y = data_split(df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(training_data)\n",
    "training_data = scaler.transform(training_data)\n",
    "dev_data = scaler.transform(dev_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate inputs to ONE single input X\n",
    "training_X = hstack([training_data, training_kw, training_desc])\n",
    "dev_X = hstack([dev_data, dev_kw, dev_desc])\n",
    "test_X = hstack([test_data, test_kw, test_desc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6742508324084351"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - sum(dev_Y) / len(dev_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC\n",
    "# model = RandomForestClassifier(n_estimators=20)\n",
    "# %time model.fit(training_X, training_Y)\n",
    "# dev_pred = model.predict(dev_X)\n",
    "# print('Accuracy: %f' % accuracy_score(dev_Y, dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 29s, sys: 172 ms, total: 5min 29s\n",
      "Wall time: 5min 30s\n",
      "Accuracy: 0.700703\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "%time model.fit(training_X, training_Y)\n",
    "dev_pred = model.predict(dev_X)\n",
    "print('Accuracy: %f' % accuracy_score(dev_Y, dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier(n_estimators=500)\n",
    "# %time model.fit(training_X, training_Y)\n",
    "# dev_pred = model.predict(dev_X)\n",
    "# print('Accuracy: %f' % accuracy_score(dev_Y, dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 s, sys: 0 ns, total: 21.8 s\n",
      "Wall time: 21.8 s\n",
      "accuracy is: 0.6712911579726231\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "model = svm.LinearSVC()\n",
    "%time model.fit(training_X, training_Y)\n",
    "dev_pred = model.predict(dev_X)\n",
    "print('accuracy is: %s' %accuracy_score(dev_Y, dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.34 s, sys: 4 ms, total: 3.34 s\n",
      "Wall time: 3.34 s\n",
      "accuracy is: 0.6727709951905291\n"
     ]
    }
   ],
   "source": [
    "# Linear Logistic Regression\n",
    "model = linear_model.LogisticRegression()\n",
    "%time model.fit(training_X, training_Y)\n",
    "dev_pred = model.predict(dev_X)\n",
    "print('accuracy is: %s' %accuracy_score(dev_Y, dev_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network\n",
    "1. Shallow NN Model\n",
    "2. RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Merge\n",
    "from keras.layers import Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import Model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = df['desc']\n",
    "kw = df['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the text features\n",
    "vocab_size = 10000\n",
    "encoded_desc = [one_hot(d, vocab_size) for d in desc]\n",
    "encoded_kw = [one_hot(d, vocab_size) for d in kw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad documents to a max length of words\n",
    "padded_desc = pad_sequences(encoded_desc, maxlen=20, padding='post')\n",
    "padded_kw = pad_sequences(encoded_kw, maxlen=5, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_desc, dev_desc, test_desc = data_split(padded_desc)\n",
    "training_kw, dev_kw, test_kw  = data_split(padded_kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(training_data)\n",
    "training_data = scaler.transform(training_data)\n",
    "dev_data = scaler.transform(dev_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_2 (Merge)              (None, 1314)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                42080     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,042,561\n",
      "Trainable params: 1,042,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "desc_model = Sequential()\n",
    "desc_model.add(Embedding(vocab_size, 50, input_length=20))\n",
    "desc_model.add(Flatten())\n",
    "\n",
    "kw_model = Sequential()\n",
    "kw_model.add(Embedding(vocab_size, 50, input_length=5))\n",
    "kw_model.add(Flatten())\n",
    "\n",
    "data_model = Sequential()\n",
    "\n",
    "inputs = Input(shape=(6,))\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64)(inputs)\n",
    "data_model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([desc_model, kw_model, data_model\n",
    "                ], mode='concat')) # input size (None, 106)\n",
    "\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 19s 193us/step - loss: 0.5977 - acc: 0.6940\n",
      "5406/5406 [==============================] - 0s 72us/step\n",
      "Accuracy: 69.348872\n",
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 18s 183us/step - loss: 0.5471 - acc: 0.7297\n",
      "5406/5406 [==============================] - 0s 61us/step\n",
      "Accuracy: 69.348872\n",
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 18s 183us/step - loss: 0.4803 - acc: 0.7890\n",
      "5406/5406 [==============================] - 0s 61us/step\n",
      "Accuracy: 67.258602\n",
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 18s 182us/step - loss: 0.3687 - acc: 0.8633\n",
      "5406/5406 [==============================] - 0s 61us/step\n",
      "Accuracy: 66.407695\n",
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 18s 183us/step - loss: 0.2657 - acc: 0.9183\n",
      "5406/5406 [==============================] - 0s 61us/step\n",
      "Accuracy: 64.002960\n",
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 18s 182us/step - loss: 0.1952 - acc: 0.9486\n",
      "5406/5406 [==============================] - 0s 62us/step\n",
      "Accuracy: 64.428413\n",
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 18s 185us/step - loss: 0.1519 - acc: 0.9640\n",
      "5406/5406 [==============================] - 0s 61us/step\n",
      "Accuracy: 64.816870\n",
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 18s 182us/step - loss: 0.1219 - acc: 0.9748\n",
      "5406/5406 [==============================] - 0s 61us/step\n",
      "Accuracy: 62.763596\n",
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 18s 183us/step - loss: 0.1042 - acc: 0.9797\n",
      "5406/5406 [==============================] - 0s 61us/step\n",
      "Accuracy: 63.706992\n",
      "Epoch 1/1\n",
      "97316/97316 [==============================] - 18s 183us/step - loss: 0.0923 - acc: 0.9832\n",
      "5406/5406 [==============================] - 0s 62us/step\n",
      "Accuracy: 62.301147\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "for _ in range(10):\n",
    "    model.fit([training_desc, training_kw, training_data,], training_Y, epochs=1, verbose=1)\n",
    "    loss, accuracy = model.evaluate([dev_desc, dev_kw, norm_dev_data], dev_Y, verbose=1)\n",
    "    print('Accuracy: %f' % (accuracy*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
